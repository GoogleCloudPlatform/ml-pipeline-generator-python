{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "End_to_End_Workflow_w_ML_Pipeline_Generator.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tauexuGq8IAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2020 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKtG6V9CBA-f",
        "colab_type": "text"
      },
      "source": [
        "# **End to End Workflow with ML Pipeline Generator**\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.sandbox.google.com/github/GoogleCloudPlatform/ml-pipeline-generator-python/examples/getting_started_notebook.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/ml-pipeline-generator-python/examples/getting_started_notebook.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SATX51N8tFga"
      },
      "source": [
        "## **Overview**\n",
        "ML Pipeline Generator simplifies model building, training and deployment by generating the required training and deployment modules for your model. Using this tool, users with locally running scripts and notebooks can get started with AI Platform and Kubeflow Pipelines in a few steps, and will have the boilerplate code needed to customize their deployments and pipelines further.\n",
        "\n",
        "[Insert Pic]\n",
        "\n",
        "This demo shows you how to train and deploy Machine Learning models on a sample dataset. The demo is divided into two parts:\n",
        "\n",
        "* Preparing an SVM classifier for training on Cloud AI platform\n",
        "* Orchestrating the training of a Tensorflow model on Kubeflow Pipelines\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gKoQObu-s1gT"
      },
      "source": [
        "### **Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ab_lae0MURrk"
      },
      "source": [
        "This tutorial uses the [United States Census Income Dataset](https://archive.ics.uci.edu/ml/datasets/census+income) provided by the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) containing information about people from a 1994 Census database, including age, education, marital status, occupation, and whether they make more than $50,000 a year. The dataset consists of over 30k rows, where each row corresponds to a different person. For a given row, there are 14 features that the model conditions on to predict the income of the person. A few of the features are named above, and the exhaustive list can be found both in the dataset link above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wLEZGISqBshz"
      },
      "source": [
        "## **Set up your local development environment**\n",
        "\n",
        "**If you are using Colab or AI Platform Notebooks**, your environment already meets\n",
        "all the requirements to run this notebook. If you are using **AI Platform Notebook**, make sure the machine configuration type is **1 vCPU, 3.75 GB RAM** or above. You can skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fzgazCAOEMv0"
      },
      "source": [
        "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
        "You need the following:\n",
        "\n",
        "* The Google Cloud SDK\n",
        "* Git\n",
        "* Python 3\n",
        "* virtualenv\n",
        "* Jupyter notebook running in a virtual environment with Python 3\n",
        "\n",
        "The Google Cloud guide to [Setting up a Python development\n",
        "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
        "installation guide](https://jupyter.org/install) provide detailed instructions\n",
        "for meeting these requirements. The following steps provide a condensed set of\n",
        "instructions:\n",
        "\n",
        "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
        "\n",
        "2. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
        "\n",
        "3. [Install\n",
        "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
        "   and create a virtual environment that uses Python 3.\n",
        "\n",
        "4. Activate that environment and run `pip install jupyter` in a shell to install\n",
        "   Jupyter.\n",
        "\n",
        "5. Run `jupyter notebook` in a shell to launch Jupyter.\n",
        "\n",
        "6. Open this notebook in the Jupyter Notebook Dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hdzNyF4iCdNI"
      },
      "source": [
        "## **Set up your GCP project**\n",
        "\n",
        "**If you do not have a GCP project then the following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a GCP project.](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
        "\n",
        "3. [Create a GCP bucket](https://cloud.google.com/storage/docs/creating-buckets) so that we can store files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ac3NIGCMVF9x"
      },
      "source": [
        "## **PIP install packages and dependencies**\n",
        "\n",
        "Install addional dependencies not installed in Notebook environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Yt6PhVG0UdF1"
      },
      "source": [
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3jK7RbsFVHBg",
        "colab": {}
      },
      "source": [
        "# Use the latest major GA version of the framework.\n",
        "! pip install --upgrade ml-pipeline-gen PyYAML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kK5JATKPNf3I"
      },
      "source": [
        "**Note:** Try installing using `sudo`, if the above command throw any permission errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A37ofoNkR-L7"
      },
      "source": [
        "`Restart` the kernel to allow the package to be imported for Jupyter Notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dr--iN2kAylZ"
      },
      "source": [
        "## **Authenticate your GCP account**\n",
        "\n",
        "**If you are using AI Platform Notebooks**, your environment is already\n",
        "authenticated. Skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3yyVCJHFSEKG"
      },
      "source": [
        "Only if you are on a local Juypter Notebook or Colab Environment, follow these steps:\n",
        "\n",
        "1. [**Create a New Service Account**](https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating_service_account_keys).\n",
        "\n",
        "3. Add the following roles: \n",
        "   **Compute Engine > Compute Admin**, **ML Engine > ML Engine Admin** and **Storage > Storage Object Admin**.\n",
        "\n",
        "4. Download a JSON file that contains your key and it will be stored in your\n",
        "local environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q5TeVHKDMOJF",
        "colab": {}
      },
      "source": [
        "# If you are on Colab, run this cell and upload your service account's\n",
        "# json key.\n",
        "import os\n",
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:    \n",
        "  from google.colab import files\n",
        "  keyfile_upload = files.upload()\n",
        "  keyfile = list(keyfile_upload.keys())[0]\n",
        "  keyfile_path = os.path.abspath(keyfile)\n",
        "  %env GOOGLE_APPLICATION_CREDENTIALS $keyfile_path\n",
        "  ! gcloud auth activate-service-account --key-file $keyfile_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmPGvXhtJGkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you are running this notebook locally, replace the string below \n",
        "# with the path to your service account key and run this cell \n",
        "# to authenticate your GCP account.\n",
        "\n",
        "%env GOOGLE_APPLICATION_CREDENTIALS /path/to/service/account\n",
        "! gcloud auth activate-service-account --key-file '/path/to/service/account'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8xAl5g5Kjhe",
        "colab_type": "text"
      },
      "source": [
        "## **Before You Begin**\n",
        "\n",
        "The tool requires following Google Cloud APIs to be enabled:\n",
        "* [Google Cloud Storage](https://cloud.google.com/storage)\n",
        "* [Cloud AI Platform](https://cloud.google.com/ai-platform)\n",
        "* [Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine)\n",
        "\n",
        "Add your Project ID below, you can change the region below if you would like, but it is not a requirement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcruGzP4K9yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PROJECT_ID = \"[PROJECT-ID]\" #@param {type:\"string\"}\n",
        "COMPUTE_REGION = \"us-central1\" # Currently only supported region."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaZ1r8svND9C",
        "colab_type": "text"
      },
      "source": [
        "Also add your bucket name:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF-XdTp7NEPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUCKET_NAME = \"[BUCKET-ID]\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuAzd_LiLWzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gcloud config set project {PROJECT_ID}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wxqgX2kK8kp",
        "colab_type": "text"
      },
      "source": [
        "The tool requires following Google Cloud APIs to be enabled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2tf_dKNK2YV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gcloud services enable ml.googleapis.com \\\n",
        "compute.googleapis.com \\\n",
        "storage-component.googleapis.com"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7nMszzEO84Q",
        "colab_type": "text"
      },
      "source": [
        "## **Create a model locally**\n",
        "\n",
        "In this section we will create a model locally, which many users have. This section is done to illustrate the on-prem method of creating models and in the next section we will show how to train them on GCP so that you can leverage the benefits of the cloud like easy distributed training, paralllel hyperparameter tuning and fast, up-to-date accelerators.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOaY0D8WUPe4",
        "colab_type": "text"
      },
      "source": [
        "The next block of code highlights how we will preprocess the census data. It is out of scope for this colab to dive into how the code works. All that is important is that the function `load_data` returns 4 values: the training features, the training predictor, the evaluation features and the evaluation predictor in that order (this function also uploads data into GCS). Run the hidden cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paMKIoJ1UHmK",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "# python3\n",
        "# Copyright 2019 Google Inc. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Train a simple TF classifier for MNIST dataset.\n",
        "\n",
        "This example comes from the cloudml-samples keras demo.\n",
        "github.com/GoogleCloudPlatform/cloudml-samples/blob/master/census/tf-keras\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "from six.moves import urllib\n",
        "import tempfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "\n",
        "DATA_DIR = os.path.join(tempfile.gettempdir(), \"census_data\")\n",
        "DATA_URL = (\"https://storage.googleapis.com/cloud-samples-data/ai-platform\"\n",
        "            + \"/census/data/\")\n",
        "TRAINING_FILE = \"adult.data.csv\"\n",
        "EVAL_FILE = \"adult.test.csv\"\n",
        "TRAINING_URL = os.path.join(DATA_URL, TRAINING_FILE)\n",
        "EVAL_URL = os.path.join(DATA_URL, EVAL_FILE)\n",
        "\n",
        "_CSV_COLUMNS = [\n",
        "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
        "    \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
        "    \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n",
        "    \"income_bracket\",\n",
        "]\n",
        "_LABEL_COLUMN = \"income_bracket\"\n",
        "UNUSED_COLUMNS = [\"fnlwgt\", \"education\", \"gender\"]\n",
        "\n",
        "_CATEGORICAL_TYPES = {\n",
        "    \"workclass\": pd.api.types.CategoricalDtype(categories=[\n",
        "        \"Federal-gov\", \"Local-gov\", \"Never-worked\", \"Private\", \"Self-emp-inc\",\n",
        "        \"Self-emp-not-inc\", \"State-gov\", \"Without-pay\"\n",
        "    ]),\n",
        "    \"marital_status\": pd.api.types.CategoricalDtype(categories=[\n",
        "        \"Divorced\", \"Married-AF-spouse\", \"Married-civ-spouse\",\n",
        "        \"Married-spouse-absent\", \"Never-married\", \"Separated\", \"Widowed\"\n",
        "    ]),\n",
        "    \"occupation\": pd.api.types.CategoricalDtype([\n",
        "        \"Adm-clerical\", \"Armed-Forces\", \"Craft-repair\", \"Exec-managerial\",\n",
        "        \"Farming-fishing\", \"Handlers-cleaners\", \"Machine-op-inspct\",\n",
        "        \"Other-service\", \"Priv-house-serv\", \"Prof-specialty\", \"Protective-serv\",\n",
        "        \"Sales\", \"Tech-support\", \"Transport-moving\"\n",
        "    ]),\n",
        "    \"relationship\": pd.api.types.CategoricalDtype(categories=[\n",
        "        \"Husband\", \"Not-in-family\", \"Other-relative\", \"Own-child\", \"Unmarried\",\n",
        "        \"Wife\"\n",
        "    ]),\n",
        "    \"race\": pd.api.types.CategoricalDtype(categories=[\n",
        "        \"Amer-Indian-Eskimo\", \"Asian-Pac-Islander\", \"Black\", \"Other\", \"White\"\n",
        "    ]),\n",
        "    \"native_country\": pd.api.types.CategoricalDtype(categories=[\n",
        "        \"Cambodia\", \"Canada\", \"China\", \"Columbia\", \"Cuba\", \"Dominican-Republic\",\n",
        "        \"Ecuador\", \"El-Salvador\", \"England\", \"France\", \"Germany\", \"Greece\",\n",
        "        \"Guatemala\", \"Haiti\", \"Holand-Netherlands\", \"Honduras\", \"Hong\",\n",
        "        \"Hungary\", \"India\", \"Iran\", \"Ireland\", \"Italy\", \"Jamaica\", \"Japan\",\n",
        "        \"Laos\", \"Mexico\", \"Nicaragua\", \"Outlying-US(Guam-USVI-etc)\", \"Peru\",\n",
        "        \"Philippines\", \"Poland\", \"Portugal\", \"Puerto-Rico\", \"Scotland\", \"South\",\n",
        "        \"Taiwan\", \"Thailand\", \"Trinadad&Tobago\", \"United-States\", \"Vietnam\",\n",
        "        \"Yugoslavia\"\n",
        "    ]),\n",
        "    \"income_bracket\": pd.api.types.CategoricalDtype(categories=[\n",
        "        \"<=50K\", \">50K\"\n",
        "    ])\n",
        "}\n",
        "\n",
        "\n",
        "def _download_and_clean_file(filename, url):\n",
        "    \"\"\"Downloads data from url, and makes changes to match the CSV format.\n",
        "\n",
        "    The CSVs may use spaces after the comma delimters (non-standard) or include\n",
        "    rows which do not represent well-formed examples. This function strips out\n",
        "    some of these problems.\n",
        "\n",
        "    Args:\n",
        "      filename: filename to save url to\n",
        "      url: URL of resource to download\n",
        "    \"\"\"\n",
        "    temp_file, _ = urllib.request.urlretrieve(url)\n",
        "    with tf.io.gfile.GFile(temp_file, \"r\") as temp_file_object:\n",
        "        with tf.io.gfile.GFile(filename, \"w\") as file_object:\n",
        "            for line in temp_file_object:\n",
        "                line = line.strip()\n",
        "                line = line.replace(\", \", \",\")\n",
        "                if not line or \",\" not in line:\n",
        "                    continue\n",
        "                if line[-1] == \".\":\n",
        "                    line = line[:-1]\n",
        "                line += \"\\n\"\n",
        "                file_object.write(line)\n",
        "    tf.io.gfile.remove(temp_file)\n",
        "\n",
        "\n",
        "def download(data_dir):\n",
        "    \"\"\"Downloads census data if it is not already present.\n",
        "\n",
        "    Args:\n",
        "      data_dir: directory where we will access/save the census data\n",
        "\n",
        "    Returns:\n",
        "      foo\n",
        "    \"\"\"\n",
        "    tf.io.gfile.makedirs(data_dir)\n",
        "\n",
        "    training_file_path = os.path.join(data_dir, TRAINING_FILE)\n",
        "    if not tf.io.gfile.exists(training_file_path):\n",
        "        _download_and_clean_file(training_file_path, TRAINING_URL)\n",
        "\n",
        "    eval_file_path = os.path.join(data_dir, EVAL_FILE)\n",
        "    if not tf.io.gfile.exists(eval_file_path):\n",
        "        _download_and_clean_file(eval_file_path, EVAL_URL)\n",
        "\n",
        "    return training_file_path, eval_file_path\n",
        "\n",
        "\n",
        "def upload(train_df, eval_df, train_path, eval_path):\n",
        "    train_df.to_csv(os.path.join(os.path.dirname(train_path), TRAINING_FILE),\n",
        "                    index=False, header=False)\n",
        "    eval_df.to_csv(os.path.join(os.path.dirname(eval_path), EVAL_FILE),\n",
        "                   index=False, header=False)\n",
        "\n",
        "\n",
        "def preprocess(dataframe):\n",
        "    \"\"\"Converts categorical features to numeric. Removes unused columns.\n",
        "\n",
        "    Args:\n",
        "      dataframe: Pandas dataframe with raw data\n",
        "\n",
        "    Returns:\n",
        "      Dataframe with preprocessed data\n",
        "    \"\"\"\n",
        "    dataframe = dataframe.drop(columns=UNUSED_COLUMNS)\n",
        "\n",
        "    # Convert integer valued (numeric) columns to floating point\n",
        "    numeric_columns = dataframe.select_dtypes([\"int64\"]).columns\n",
        "    dataframe[numeric_columns] = dataframe[numeric_columns].astype(\"float32\")\n",
        "\n",
        "    # Convert categorical columns to numeric\n",
        "    cat_columns = dataframe.select_dtypes([\"object\"]).columns\n",
        "    dataframe[cat_columns] = dataframe[cat_columns].apply(\n",
        "        lambda x: x.astype(_CATEGORICAL_TYPES[x.name]))\n",
        "    dataframe[cat_columns] = dataframe[cat_columns].apply(\n",
        "        lambda x: x.cat.codes)\n",
        "    return dataframe\n",
        "\n",
        "\n",
        "def standardize(dataframe):\n",
        "    \"\"\"Scales numerical columns using their means and standard deviation.\n",
        "\n",
        "    Args:\n",
        "      dataframe: Pandas dataframe\n",
        "\n",
        "    Returns:\n",
        "      Input dataframe with the numerical columns scaled to z-scores\n",
        "    \"\"\"\n",
        "    dtypes = list(zip(dataframe.dtypes.index, map(str, dataframe.dtypes)))\n",
        "    for column, dtype in dtypes:\n",
        "        if dtype == \"float32\":\n",
        "            dataframe[column] -= dataframe[column].mean()\n",
        "            dataframe[column] /= dataframe[column].std()\n",
        "    return dataframe\n",
        "\n",
        "\n",
        "def load_data(train_path=\"\", eval_path=\"\"):\n",
        "    \"\"\"Loads data into preprocessed (train_x, train_y, eval_y, eval_y) dataframes.\n",
        "\n",
        "    Args:\n",
        "      train_path: Local or GCS path to uploaded train data to.\n",
        "      eval_path: Local or GCS path to uploaded eval data to.\n",
        "\n",
        "    Returns:\n",
        "      A tuple (train_x, train_y, eval_x, eval_y), where train_x and eval_x are\n",
        "      Pandas dataframes with features for training and train_y and eval_y are\n",
        "      numpy arrays with the corresponding labels.\n",
        "    \"\"\"\n",
        "    # Download Census dataset: Training and eval csv files.\n",
        "    training_file_path, eval_file_path = download(DATA_DIR)\n",
        "\n",
        "    train_df = pd.read_csv(\n",
        "        training_file_path, names=_CSV_COLUMNS, na_values=\"?\")\n",
        "    eval_df = pd.read_csv(eval_file_path, names=_CSV_COLUMNS, na_values=\"?\")\n",
        "\n",
        "    train_df = preprocess(train_df)\n",
        "    eval_df = preprocess(eval_df)\n",
        "\n",
        "    # Split train and eval data with labels. The pop method copies and removes\n",
        "    # the label column from the dataframe.\n",
        "    train_x, train_y = train_df, train_df.pop(_LABEL_COLUMN)\n",
        "    eval_x, eval_y = eval_df, eval_df.pop(_LABEL_COLUMN)\n",
        "\n",
        "    # Join train_x and eval_x to normalize on overall means and standard\n",
        "    # deviations. Then separate them again.\n",
        "    all_x = pd.concat([train_x, eval_x], keys=[\"train\", \"eval\"])\n",
        "    all_x = standardize(all_x)\n",
        "    train_x, eval_x = all_x.xs(\"train\"), all_x.xs(\"eval\")\n",
        "\n",
        "    # Rejoin features and labels and upload to GCS.\n",
        "    if train_path and eval_path:\n",
        "        train_df = train_x.copy()\n",
        "        train_df[_LABEL_COLUMN] = train_y\n",
        "        eval_df = eval_x.copy()\n",
        "        eval_df[_LABEL_COLUMN] = eval_y\n",
        "        upload(train_df, eval_df, train_path, eval_path)\n",
        "\n",
        "    # Reshape label columns for use with tf.data.Dataset\n",
        "    train_y = np.asarray(train_y).astype(\"float32\").reshape((-1, 1))\n",
        "    eval_y = np.asarray(eval_y).astype(\"float32\").reshape((-1, 1))\n",
        "\n",
        "    return train_x, train_y, eval_x, eval_y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94gH19oBVx8z",
        "colab_type": "text"
      },
      "source": [
        "Now we train the a sklearn SVM model on this data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaSFB8ZRUNHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm\n",
        "\n",
        "train_x, train_y, eval_x, eval_y = load_data()\n",
        "train_y, eval_y = [np.ravel(x) for x in [train_y, eval_y]]\n",
        "classifier = svm.SVC(C=1)\n",
        "classifier.fit(train_x, train_y)\n",
        "score = classifier.score(eval_x, eval_y)\n",
        "print('Accuracy is {}'.format(score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWZGLhymV7rp",
        "colab_type": "text"
      },
      "source": [
        "Usually, the pipelines have more complexities to it, such as hyperparameter tuning. However, at the end we have a single model which is the best and which we want to serve in production."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsCVB4HeWiCm",
        "colab_type": "text"
      },
      "source": [
        "## Preparing an SVM classifier for training on Cloud AI platform\n",
        "\n",
        "We now have a model which we think is good, but we want to add this model onto GCP while at the same time adding additional features such as training and prediction so future runs will be simple."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spRgKIofaHG1",
        "colab_type": "text"
      },
      "source": [
        "We can leverage the examples that are in thie ML Pipeline Generator as they give good examples and templates to follow. So first we clone the github repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c76t59CjaR5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/GoogleCloudPlatform/ml-pipeline-generator-python.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k7wecnxa5BM",
        "colab_type": "text"
      },
      "source": [
        "Then we copy the sklearn example to the current directory and go into this folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9EOGviSaXZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r ml-pipeline-generator-python/examples/sklearn sklearn-demo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tJf9xn0anlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd sklearn-demo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwmCi-OScYcG",
        "colab_type": "text"
      },
      "source": [
        "We now modify the config.yaml.example file with out project id, bucket id and model name. Note the training and evaluation data files should be stored in your bucket already, unless you decided to handle that upload in your preprocessing function (like in this lab)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2hyf565V7K_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile config.yaml\n",
        "# Copyright 2020 Google Inc. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "#\n",
        "# Config file for ML Pipeline Generator.\n",
        "\n",
        "project_id: [PROJECT ID]\n",
        "bucket_id: [BUCKET ID]\n",
        "region: \"us-central1\"\n",
        "scale_tier: \"STANDARD_1\"\n",
        "runtime_version: \"1.15\"\n",
        "python_version: \"3.7\"\n",
        "package_name: \"ml_pipeline_gen\"\n",
        "machine_type_pred: \"mls1-c4-m2\"\n",
        "\n",
        "data:\n",
        "    schema:\n",
        "        - \"age\"\n",
        "        - \"workclass\"\n",
        "        - \"education_num\"\n",
        "        - \"marital_status\"\n",
        "        - \"occupation\"\n",
        "        - \"relationship\"\n",
        "        - \"race\"\n",
        "        - \"capital_gain\"\n",
        "        - \"capital_loss\"\n",
        "        - \"hours_per_week\"\n",
        "        - \"native_country\"\n",
        "        - \"income_bracket\"\n",
        "    train: \"gs://[BUCKET ID]/[MODEL NAME]/data/adult.data.csv\"\n",
        "    evaluation: \"gs://[BUCKET ID]/[MODEL NAME]/data/adult.test.csv\"\n",
        "    prediction:\n",
        "        input_data_paths:\n",
        "            - \"gs://[BUCKET ID]/[MODEL NAME]/inputs/*\"\n",
        "        input_format: \"JSON\"\n",
        "        output_format: \"JSON\"\n",
        "\n",
        "model:\n",
        "    # Name must start with a letter and only contain letters, numbers, and\n",
        "    # underscores.\n",
        "    name: [MODEL NAME]\n",
        "    path: \"model.sklearn_model\"\n",
        "    target: \"income_bracket\"\n",
        "\n",
        "model_params:\n",
        "    input_args:\n",
        "        C:\n",
        "            type: \"float\"\n",
        "            help: \"Regularization parameter, must be positive.\"\n",
        "            default: 1.0\n",
        "    # Relative path.\n",
        "    hyperparam_config: \"hptuning_config.yaml\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBu-HcVOc98e",
        "colab_type": "text"
      },
      "source": [
        "We now copy our previous preoprocessing code into the file `concensus_preprocess.py`. Run the hidden cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lxlrq6NVkVV",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "%%writefile model/census_preprocess.py\n",
        "# python3\n",
        "# Copyright 2019 Google Inc. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Train a simple TF classifier for MNIST dataset.\n",
        "\n",
        "This example comes from the cloudml-samples keras demo.\n",
        "github.com/GoogleCloudPlatform/cloudml-samples/blob/master/census/tf-keras\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "from six.moves import urllib\n",
        "import tempfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "\n",
        "DATA_DIR = os.path.join(tempfile.gettempdir(), \"census_data\")\n",
        "DATA_URL = (\"https://storage.googleapis.com/cloud-samples-data/ai-platform\"\n",
        "            + \"/census/data/\")\n",
        "TRAINING_FILE = \"adult.data.csv\"\n",
        "EVAL_FILE = \"adult.test.csv\"\n",
        "TRAINING_URL = os.path.join(DATA_URL, TRAINING_FILE)\n",
        "EVAL_URL = os.path.join(DATA_URL, EVAL_FILE)\n",
        "\n",
        "_CSV_COLUMNS = [\n",
        "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
        "    \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
        "    \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n",
        "    \"income_bracket\",\n",
        "]\n",
        "_LABEL_COLUMN = \"income_bracket\"\n",
        "UNUSED_COLUMNS = [\"fnlwgt\", \"education\", \"gender\"]\n",
        "\n",
        "_CATEGORICAL_TYPES = {\n",
        "    \"workclass\": pd.api.types.CategoricalDtype(categories=[\n",
        "        \"Federal-gov\", \"Local-gov\", \"Never-worked\", \"Private\", \"Self-emp-inc\",\n",
        "        \"Self-emp-not-inc\", \"State-gov\", \"Without-pay\"\n",
        "    ]),\n",
        "    \"marital_status\": pd.api.types.CategoricalDtype(categories=[\n",
        "        \"Divorced\", \"Married-AF-spouse\", \"Married-civ-spouse\",\n",
        "        \"Married-spouse-absent\", \"Never-married\", \"Separated\", \"Widowed\"\n",
        "    ]),\n",
        "    \"occupation\": pd.api.types.CategoricalDtype([\n",
        "        \"Adm-clerical\", \"Armed-Forces\", \"Craft-repair\", \"Exec-managerial\",\n",
        "        \"Farming-fishing\", \"Handlers-cleaners\", \"Machine-op-inspct\",\n",
        "        \"Other-service\", \"Priv-house-serv\", \"Prof-specialty\", \"Protective-serv\",\n",
        "        \"Sales\", \"Tech-support\", \"Transport-moving\"\n",
        "    ]),\n",
        "    \"relationship\": pd.api.types.CategoricalDtype(categories=[\n",
        "        \"Husband\", \"Not-in-family\", \"Other-relative\", \"Own-child\", \"Unmarried\",\n",
        "        \"Wife\"\n",
        "    ]),\n",
        "    \"race\": pd.api.types.CategoricalDtype(categories=[\n",
        "        \"Amer-Indian-Eskimo\", \"Asian-Pac-Islander\", \"Black\", \"Other\", \"White\"\n",
        "    ]),\n",
        "    \"native_country\": pd.api.types.CategoricalDtype(categories=[\n",
        "        \"Cambodia\", \"Canada\", \"China\", \"Columbia\", \"Cuba\", \"Dominican-Republic\",\n",
        "        \"Ecuador\", \"El-Salvador\", \"England\", \"France\", \"Germany\", \"Greece\",\n",
        "        \"Guatemala\", \"Haiti\", \"Holand-Netherlands\", \"Honduras\", \"Hong\",\n",
        "        \"Hungary\", \"India\", \"Iran\", \"Ireland\", \"Italy\", \"Jamaica\", \"Japan\",\n",
        "        \"Laos\", \"Mexico\", \"Nicaragua\", \"Outlying-US(Guam-USVI-etc)\", \"Peru\",\n",
        "        \"Philippines\", \"Poland\", \"Portugal\", \"Puerto-Rico\", \"Scotland\", \"South\",\n",
        "        \"Taiwan\", \"Thailand\", \"Trinadad&Tobago\", \"United-States\", \"Vietnam\",\n",
        "        \"Yugoslavia\"\n",
        "    ]),\n",
        "    \"income_bracket\": pd.api.types.CategoricalDtype(categories=[\n",
        "        \"<=50K\", \">50K\"\n",
        "    ])\n",
        "}\n",
        "\n",
        "\n",
        "def _download_and_clean_file(filename, url):\n",
        "    \"\"\"Downloads data from url, and makes changes to match the CSV format.\n",
        "\n",
        "    The CSVs may use spaces after the comma delimters (non-standard) or include\n",
        "    rows which do not represent well-formed examples. This function strips out\n",
        "    some of these problems.\n",
        "\n",
        "    Args:\n",
        "      filename: filename to save url to\n",
        "      url: URL of resource to download\n",
        "    \"\"\"\n",
        "    temp_file, _ = urllib.request.urlretrieve(url)\n",
        "    with tf.io.gfile.GFile(temp_file, \"r\") as temp_file_object:\n",
        "        with tf.io.gfile.GFile(filename, \"w\") as file_object:\n",
        "            for line in temp_file_object:\n",
        "                line = line.strip()\n",
        "                line = line.replace(\", \", \",\")\n",
        "                if not line or \",\" not in line:\n",
        "                    continue\n",
        "                if line[-1] == \".\":\n",
        "                    line = line[:-1]\n",
        "                line += \"\\n\"\n",
        "                file_object.write(line)\n",
        "    tf.io.gfile.remove(temp_file)\n",
        "\n",
        "\n",
        "def download(data_dir):\n",
        "    \"\"\"Downloads census data if it is not already present.\n",
        "\n",
        "    Args:\n",
        "      data_dir: directory where we will access/save the census data\n",
        "\n",
        "    Returns:\n",
        "      foo\n",
        "    \"\"\"\n",
        "    tf.io.gfile.makedirs(data_dir)\n",
        "\n",
        "    training_file_path = os.path.join(data_dir, TRAINING_FILE)\n",
        "    if not tf.io.gfile.exists(training_file_path):\n",
        "        _download_and_clean_file(training_file_path, TRAINING_URL)\n",
        "\n",
        "    eval_file_path = os.path.join(data_dir, EVAL_FILE)\n",
        "    if not tf.io.gfile.exists(eval_file_path):\n",
        "        _download_and_clean_file(eval_file_path, EVAL_URL)\n",
        "\n",
        "    return training_file_path, eval_file_path\n",
        "\n",
        "\n",
        "def upload(train_df, eval_df, train_path, eval_path):\n",
        "    train_df.to_csv(os.path.join(os.path.dirname(train_path), TRAINING_FILE),\n",
        "                    index=False, header=False)\n",
        "    eval_df.to_csv(os.path.join(os.path.dirname(eval_path), EVAL_FILE),\n",
        "                   index=False, header=False)\n",
        "\n",
        "\n",
        "def preprocess(dataframe):\n",
        "    \"\"\"Converts categorical features to numeric. Removes unused columns.\n",
        "\n",
        "    Args:\n",
        "      dataframe: Pandas dataframe with raw data\n",
        "\n",
        "    Returns:\n",
        "      Dataframe with preprocessed data\n",
        "    \"\"\"\n",
        "    dataframe = dataframe.drop(columns=UNUSED_COLUMNS)\n",
        "\n",
        "    # Convert integer valued (numeric) columns to floating point\n",
        "    numeric_columns = dataframe.select_dtypes([\"int64\"]).columns\n",
        "    dataframe[numeric_columns] = dataframe[numeric_columns].astype(\"float32\")\n",
        "\n",
        "    # Convert categorical columns to numeric\n",
        "    cat_columns = dataframe.select_dtypes([\"object\"]).columns\n",
        "    dataframe[cat_columns] = dataframe[cat_columns].apply(\n",
        "        lambda x: x.astype(_CATEGORICAL_TYPES[x.name]))\n",
        "    dataframe[cat_columns] = dataframe[cat_columns].apply(\n",
        "        lambda x: x.cat.codes)\n",
        "    return dataframe\n",
        "\n",
        "\n",
        "def standardize(dataframe):\n",
        "    \"\"\"Scales numerical columns using their means and standard deviation.\n",
        "\n",
        "    Args:\n",
        "      dataframe: Pandas dataframe\n",
        "\n",
        "    Returns:\n",
        "      Input dataframe with the numerical columns scaled to z-scores\n",
        "    \"\"\"\n",
        "    dtypes = list(zip(dataframe.dtypes.index, map(str, dataframe.dtypes)))\n",
        "    for column, dtype in dtypes:\n",
        "        if dtype == \"float32\":\n",
        "            dataframe[column] -= dataframe[column].mean()\n",
        "            dataframe[column] /= dataframe[column].std()\n",
        "    return dataframe\n",
        "\n",
        "\n",
        "def load_data(train_path=\"\", eval_path=\"\"):\n",
        "    \"\"\"Loads data into preprocessed (train_x, train_y, eval_y, eval_y) dataframes.\n",
        "\n",
        "    Args:\n",
        "      train_path: Local or GCS path to uploaded train data to.\n",
        "      eval_path: Local or GCS path to uploaded eval data to.\n",
        "\n",
        "    Returns:\n",
        "      A tuple (train_x, train_y, eval_x, eval_y), where train_x and eval_x are\n",
        "      Pandas dataframes with features for training and train_y and eval_y are\n",
        "      numpy arrays with the corresponding labels.\n",
        "    \"\"\"\n",
        "    # Download Census dataset: Training and eval csv files.\n",
        "    training_file_path, eval_file_path = download(DATA_DIR)\n",
        "\n",
        "    train_df = pd.read_csv(\n",
        "        training_file_path, names=_CSV_COLUMNS, na_values=\"?\")\n",
        "    eval_df = pd.read_csv(eval_file_path, names=_CSV_COLUMNS, na_values=\"?\")\n",
        "\n",
        "    train_df = preprocess(train_df)\n",
        "    eval_df = preprocess(eval_df)\n",
        "\n",
        "    # Split train and eval data with labels. The pop method copies and removes\n",
        "    # the label column from the dataframe.\n",
        "    train_x, train_y = train_df, train_df.pop(_LABEL_COLUMN)\n",
        "    eval_x, eval_y = eval_df, eval_df.pop(_LABEL_COLUMN)\n",
        "\n",
        "    # Join train_x and eval_x to normalize on overall means and standard\n",
        "    # deviations. Then separate them again.\n",
        "    all_x = pd.concat([train_x, eval_x], keys=[\"train\", \"eval\"])\n",
        "    all_x = standardize(all_x)\n",
        "    train_x, eval_x = all_x.xs(\"train\"), all_x.xs(\"eval\")\n",
        "\n",
        "    # Rejoin features and labels and upload to GCS.\n",
        "    if train_path and eval_path:\n",
        "        train_df = train_x.copy()\n",
        "        train_df[_LABEL_COLUMN] = train_y\n",
        "        eval_df = eval_x.copy()\n",
        "        eval_df[_LABEL_COLUMN] = eval_y\n",
        "        upload(train_df, eval_df, train_path, eval_path)\n",
        "\n",
        "    # Reshape label columns for use with tf.data.Dataset\n",
        "    train_y = np.asarray(train_y).astype(\"float32\").reshape((-1, 1))\n",
        "    eval_y = np.asarray(eval_y).astype(\"float32\").reshape((-1, 1))\n",
        "\n",
        "    return train_x, train_y, eval_x, eval_y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wjQu8yxeByD",
        "colab_type": "text"
      },
      "source": [
        "We perform a similar copy and paste into the `sklearn_model.py` file, with the addition of a parameter `C` which we will use for hyperparameter tuning. You can add as much hyperparameters as you requre to tune."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4Jh2oK-d4LC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile model/sklearn_model.py\n",
        "# python3\n",
        "# Copyright 2019 Google Inc. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Train a simple SVM classifier.\"\"\"\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "from sklearn import svm\n",
        "\n",
        "from model.census_preprocess import load_data\n",
        "\n",
        "\n",
        "def get_model(params):\n",
        "    \"\"\"Trains a classifier.\"\"\"\n",
        "    classifier = svm.SVC(C=params.C)\n",
        "    return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev2stgwseZfc",
        "colab_type": "text"
      },
      "source": [
        "We now speify the hyperparameters for our training runs based on the [hyperparameter tuning yaml format](https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning) for CAIP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiRqisEXd4IY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile hptuning_config.yaml\n",
        "# Copyright 2020 Google Inc. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "trainingInput:\n",
        "  scaleTier: STANDARD_1\n",
        "  hyperparameters:\n",
        "    goal: MAXIMIZE\n",
        "    maxTrials: 2\n",
        "    maxParallelTrials: 2\n",
        "    hyperparameterMetricTag: score\n",
        "    enableTrialEarlyStopping: TRUE\n",
        "    params:\n",
        "    - parameterName: C\n",
        "      type: DOUBLE\n",
        "      minValue: .001\n",
        "      maxValue: 10\n",
        "      scaleType: UNIT_LOG_SCALE\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbDq0EKkfUOS",
        "colab_type": "text"
      },
      "source": [
        "## Run the Sklearn Model on CAIP\n",
        "\n",
        "We only modified two yaml files and the `demo.py` file to specify training, hyperparameter tuning and model prediction. Then, we simply copied and pasted our existing code for preprocessing and building the model. We did not have to write any GCP specific code as yet, this will all be handled by this solution. Now we can submit our jobs to the cloud with a few commands"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Ierc5Md4Fj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ml_pipeline_gen.models import SklearnModel\n",
        "from model.census_preprocess import load_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbqoMgBAcNIw",
        "colab_type": "text"
      },
      "source": [
        "Specify the path of your `config.yaml` file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JcdbjdkcSWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = \"config.yaml\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLuvsdCVcS9B",
        "colab_type": "text"
      },
      "source": [
        "Now, we can easily create our model, generate all the necessary Cloud AI Platform files needed to train the model, upload the data files and train the model in 4 simple commands. Note, our `load_data` function uploads the files for us automatically, you can also manually upload the files to the buckets you specified in the `config.yaml` file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv2GaGszfSkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = SklearnModel(config)\n",
        "model.generate_files()\n",
        "\n",
        "# this fn is from out preprocessing file and\n",
        "# automatically uploads our data to GCS\n",
        "load_data(model.data[\"train\"], model.data[\"evaluation\"])\n",
        "\n",
        "job_id = model.train(tune=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCjUgLxScx0w",
        "colab_type": "text"
      },
      "source": [
        "After training, we would like to test our model's prediction. First, deploy the model (our code automatically returns a generated version). Then request online predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZDLwVTwd37q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_input = [\n",
        "    [0.02599666, 6, 1.1365801, 4, 0, 1, 4, 0.14693314, -0.21713187,\n",
        "      -0.034039237, 38],\n",
        "]\n",
        "version = model.deploy(job_id=job_id)\n",
        "preds = model.online_predict(pred_input, version=version)\n",
        "\n",
        "print(\"Features: {}\".format(pred_input))\n",
        "print(\"Predictions: {}\".format(preds))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}